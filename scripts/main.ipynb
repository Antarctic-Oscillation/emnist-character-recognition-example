{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "import torchvision.utils as vutils\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "from utils import save_checkpoint, load_checkpoint\n",
    "\n",
    "def set_seed(seed):\n",
    "    \"\"\"Set seed for reproducibility.\"\"\"\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    \n",
    "set_seed(42)\n",
    "torch.backends.cudnn.benchmark = True\n",
    "\n",
    "# Data loading code remains the same\n",
    "def get_data_loaders(batch_size: int, num_workers: int = 4):\n",
    "    \"\"\"\n",
    "    Create data loaders for EMNIST dataset.\n",
    "    \"\"\"\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.1751,), (0.3332,))  # EMNIST mean and std\n",
    "    ])\n",
    "    \n",
    "    # Using balanced split (47 classes)\n",
    "    train_dataset = datasets.EMNIST(root='./data', split='balanced', train=True, download=True, transform=transform)\n",
    "    val_dataset = datasets.EMNIST(root='./data', split='balanced', train=False, download=True, transform=transform)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=num_workers, pin_memory=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=num_workers, pin_memory=True)\n",
    "    \n",
    "    return train_loader, val_loader\n",
    "\n",
    "def visualize_batch(dataloader):\n",
    "    inputs, _ = next(iter(dataloader))\n",
    "    out = vutils.make_grid(inputs)\n",
    "\n",
    "    plt.figure(figsize=(15, 15))\n",
    "    if out.shape[0] == 1:\n",
    "        plt.imshow(out.squeeze().cpu(), cmap='gray')\n",
    "    else:\n",
    "        plt.imshow(out.permute(1, 2, 0).cpu())\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "# Hyperparameters\n",
    "NUM_CLASSES : int = 47  # EMNIST balanced split has 47 classes\n",
    "BATCH_SIZE : int= 64\n",
    "NUM_EPOCHS : int = 20\n",
    "LEARNING_RATE : float = 0.001\n",
    "CHECKPOINT_DIR : str = 'checkpoints'\n",
    "LATEST_MODEL : str = 'latest_model_checkpoint.pth'\n",
    "BEST_MODEL : str = 'best_model_checkpoint.pth'\n",
    "\n",
    "\n",
    "train_loader, val_loader = get_data_loaders(BATCH_SIZE)\n",
    "print(f'number of training images: {len(train_loader.dataset)}')\n",
    "print(f'number of validation images: {len(val_loader.dataset)}')\n",
    "\n",
    "visualize_batch(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cnn_model import ResidualCNN\n",
    "\n",
    "def train_model(\n",
    "    model: nn.Module,\n",
    "    train_loader: DataLoader,\n",
    "    val_loader: DataLoader,\n",
    "    num_epochs: int,\n",
    "    learning_rate: float,\n",
    "    device: str = \"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
    "    checkpoint_dir: str = 'checkpoints'\n",
    ") -> dict:\n",
    "    \"\"\"\n",
    "    Train the model with checkpoint support.\n",
    "    \"\"\"\n",
    "    model = model.to(device)\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=50)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    # Load checkpoint if available\n",
    "    start_epoch, train_losses, train_accuracies, val_losses, val_accuracies, best_val_accuracy = load_checkpoint(\n",
    "        model, optimizer, scheduler, checkpoint_dir, model_name=LATEST_MODEL\n",
    "    )\n",
    "    \n",
    "    history = {\n",
    "        'train_loss': train_losses,\n",
    "        'train_acc': train_accuracies,\n",
    "        'val_loss': val_losses,\n",
    "        'val_acc': val_accuracies\n",
    "    }\n",
    "    \n",
    "    for epoch in range(start_epoch, num_epochs):\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        pbar = tqdm(train_loader, desc=f'Epoch {epoch+1}/{num_epochs}')\n",
    "        for inputs, targets in pbar:\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += targets.size(0)\n",
    "            correct += predicted.eq(targets).sum().item()\n",
    "            \n",
    "            pbar.set_postfix({'loss': train_loss/len(train_loader), 'acc': 100.*correct/total})\n",
    "        \n",
    "        train_acc = 100. * correct / total\n",
    "        train_loss = train_loss / len(train_loader)\n",
    "        \n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for inputs, targets in val_loader:\n",
    "                inputs, targets = inputs.to(device), targets.to(device)\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, targets)\n",
    "                \n",
    "                val_loss += loss.item()\n",
    "                _, predicted = outputs.max(1)\n",
    "                total += targets.size(0)\n",
    "                correct += predicted.eq(targets).sum().item()\n",
    "        \n",
    "        val_acc = 100. * correct / total\n",
    "        val_loss = val_loss / len(val_loader)\n",
    "        \n",
    "        # Update learning rate\n",
    "        scheduler.step(val_loss)\n",
    "        \n",
    "        # Update history\n",
    "        history['train_loss'].append(train_loss)\n",
    "        history['train_acc'].append(train_acc)\n",
    "        history['val_loss'].append(val_loss)\n",
    "        history['val_acc'].append(val_acc)\n",
    "        \n",
    "        # Check if this is the best model\n",
    "        is_best = val_acc > best_val_accuracy\n",
    "        if is_best:\n",
    "            best_val_accuracy = val_acc\n",
    "        \n",
    "        # Save checkpoint\n",
    "        save_checkpoint(\n",
    "            epoch + 1,  # Save next epoch number\n",
    "            model,\n",
    "            optimizer,\n",
    "            scheduler,  # Pass the scheduler here\n",
    "            history['train_loss'],\n",
    "            history['train_acc'],\n",
    "            history['val_loss'],\n",
    "            history['val_acc'],\n",
    "            best_val_accuracy,\n",
    "            is_best,\n",
    "            checkpoint_dir,\n",
    "            latest_path=LATEST_MODEL,\n",
    "            best_path=LATEST_MODEL\n",
    "        )\n",
    "        \n",
    "        print(f'Epoch {epoch+1}/{num_epochs}:')\n",
    "        print(f'Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.2f}%')\n",
    "        print(f'Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.2f}%\\n')\n",
    "    \n",
    "    return history\n",
    "\n",
    "# Create model and data loaders\n",
    "model = ResidualCNN(\n",
    "    num_classes=NUM_CLASSES,\n",
    "    initial_channels=64,\n",
    "    num_layers=3,\n",
    "    dropout_rate=0.5\n",
    ")\n",
    "\n",
    "# Train model\n",
    "history = train_model(\n",
    "    model=model,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    num_epochs=NUM_EPOCHS,\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    checkpoint_dir=CHECKPOINT_DIR\n",
    ")\n",
    "\n",
    "print(\"Training completed!\")\n",
    "print(f\"Best validation accuracy: {max(history['val_acc']):.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(8, 14))\n",
    "\n",
    "# Loss vs. Epoch\n",
    "ax1.plot(history['train_loss'], label='Training Loss')\n",
    "ax1.plot(history['val_loss'], label='Validation Loss')\n",
    "ax1.set_xlabel('Epoch')\n",
    "ax1.set_ylabel('Loss')\n",
    "ax1.legend()\n",
    "ax1.set_title('a) Loss vs. Epoch')\n",
    "ax1.set_ylim(0.0, 1.5)\n",
    "\n",
    "# Accuracy vs. Epoch\n",
    "ax2.plot(history['train_acc'], label='Training Accuracy')\n",
    "ax2.plot(history['val_acc'], label='Validation Accuracy')\n",
    "ax2.set_xlabel('Epoch')\n",
    "ax2.set_ylabel('Accuracy')\n",
    "ax2.legend()\n",
    "ax2.set_title('b) Accuracy vs. Epoch')\n",
    "ax2.set_ylim(0.0, 100)\n",
    "\n",
    "# Adjust the layout and display the plot\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "char-recognition-7fxJi45j-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
